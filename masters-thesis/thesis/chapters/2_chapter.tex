% SPDX-License-Identifier: CC-BY-4.0
%
% Copyright (c) 2023 Nelson Vieira
%
% @author Nelson Vieira <2080511@student.uma.pt>
% @license CC-BY-4.0 <https://creativecommons.org/licenses/by/4.0/legalcode.txt>
\section{State of the Art} \label{section:state_of_the_art}

\par
This section provides an overview of the recent literature with the themes
that were found to be more relevant for this work.

\subsection{Privacy Paradox}

The use of a variety of digital devices has numerous advantages, but they
also bring with them the ubiquity of data capturing equipment, therefore,
it is understandable why the majority of online users have serious concerns
about the privacy of their personal data. However, the opinions expressed
are starkly at odds with the reality, according to Thomson et al. \cite{DarrenState}
report on the state of privacy, that just one in four European users read
the terms and conditions in their entirety prior to making an online purchase
or subscribing to a service, 59\% admitted to only quickly scanning the
terms and conditions before completing a purchase, while 14\% admitted to
never reading them at all, 30\% of the respondents would even swap their
email address to win a reward, or entry into a raffle, while 17\% would do
so to get an app and 30\% would do it for money.

This is what is called a privacy paradox, there have been multiple papers
written on this subject \cite{solove2021myth, WilliamsPrivacy, lee2021investigating, goad2021privacy, gerber2018explaining},
some papers attempt a theoretical explanation while others attempt an empirical
one. There has been very different interpretations or explanations of this
paradox, a few papers \cite{wilson2012unpacking, warshaw2015can, lee2015privacy}
apply the theoretical concept of the \textit{homo economicus} \cite{zak2008moral},
which is the representation of people as beings who constantly act in a
way that is logical and self-interested, not worrying about morality or
ethics, and who do so to the best of their ability, to the context of privacy.
Individuals' decision-making processes can be greatly influenced by a variety
of cognitive biases and heuristics, and these decisions can vary greatly from
individual to individual, according to several studies on individual choice behaviour \cite{knijnenburg2013dimensionality, wakefield2013influence, flender2012type}.
According to several articles \cite{dienlin2015privacy, baek2014solving},
this paradox might be explained by the fact that some people have genuinely
experienced online privacy assaults and that most privacy views are therefore
based on heuristics or second-hand accounts. Taddicken's study \cite{taddicken2014privacy}
argues that peer pressure is the reason people have this contradictory behaviour,
Norberg et al. \cite{norberg2007privacy} explains this paradox by suggesting
that while perceived risk affects reported attitudes and behavioural intentions,
trust has a direct impact on privacy behaviour, while others \cite{flender2012type, kokolakis2017privacy}
rely on quantum theory, meaning there is indeterminacy in the decision making,
in other words, it means that an individual's decision's result can be determined
at the time of the decision and not before. Brandimarte et al. \cite{brandimarte2013misplaced}
have explored the idea that when it comes to their data privacy, users have
an \textit{illusion of control}.

This paradox has been proven to be vitiated by a number of empirical studies \cite{dienlin2015privacy, xie2019consumers, SCHWAIG20131, sannon2018privacy},
online privacy practices are founded on separate privacy mindsets and so
they are not inherently paradoxical.

Another concept worth analysing is differential privacy (DP) which relates more
closely to the survey that will be conducted but also to the general
collection and analysis of user data by applications and systems.

\subsection{Differential Privacy}

The notion of differential privacy, according to Michael Kearns \cite{kearns2019ethical},
is based on three important principles. The first being that ``differential
privacy requires that adding or removing the data record of a single individual
not change the probability of any outcome by much''. The second principle
being that ``no outside observer can learn very much about any individual
because of that person's specific data''. And the third important principle
being that ``for every individual in the dataset, and for any observer no
matter what their initial beliefs about the world were, after observing
the output of a differentially private computation, their posterior belief
about anything is close to what it would have been had they observed the
output of the same computation run without the individual's data''.

DP has the potential to significantly increase individual
privacy protection, by purposefully adding noise into a dataset, it gives
plausible deniability to any individual who may have had their data exploited
while still being able to calculate statistics with relatively high precision.
Although algorithms that deal with notions of fairness, ethics, and privacy
are hard to implement because of the subjectivity of these concepts, and
DP algorithms are no exception, they can still help in
regards to tackling technology's intrinsic ethical and moral issues.

Zhao and Chen \cite{ZhaoSurvey} conduct a SLR on DP for
unstructured data. The authors present DP methods for
sensitive content in image, audio, video, and text data. They compare the
various methods and perform utility analyses for each method, highlighting
the benefits and drawbacks of each, the utility loss is measured in experimental
evaluations between the actual data and its obfuscated variant. They come
to the conclusion that DP as well as its variations give
stringent privacy protections for unstructured data against attackers with
unpredictable background knowledge. They also suggest potential future study
subjects that have yet to be investigated.

Integrating federated learning and local differential privacy (LDP) is proposed
by Zhao et al. \cite{zhao2020local} to facilitate crowdsourcing applications to
achieve the machine learning model while avoiding privacy threats and reducing
communication costs. The authors provide four mechanisms depending on the
privacy budget that is wanted. Based on experimental results of real-world
datasets, the suggested mechanisms provide stronger privacy protection when
compared to other comparable mechanisms; nevertheless, further testing is
required to establish the validity of these mechanisms on production systems.
No future research subjects are provided, only the intention of applying
these mechanisms to deep neural networks.

Similar to differential privacy, there exists different algorithms that aim to
preserve privacy such as Google's box blurring algorithm \cite{FromeLarge}
that is used in the Google Map's street view, Microsoft's Visor \cite{poddar2020visor}
which is a video-analytics-as-a-service tool and Shokri and Shmatikov's
\cite{ShokriPrivacy} system for collaborative deep learning, however, in
general, these algorithms struggle with high computational cost, internal
attacks, or non-provable privacy.

\subsection{Privacy in IoT: Approaches}

There have been a number of SLRs \cite{Gupta2022Privacy, Kuhtreiber2022survey, sicari2015security, LinSurvey, yang2022overview, zubaydi2023leveraging}
and systematic mapping reviews \cite{porras2018security, ahmed2019aspects}
done to study privacy and security issues in IoT.

In Gupta and Ghanavati's \cite{Gupta2022Privacy} SLR, the authors review
papers with methodologies and techniques that identify privacy risks or
notify users about these risks. They divide the literature into the semantic-based,
data-driven, source code analysis, survey, blockchain and architectural
and framework-based approaches. The authors' findings show that: most works concentrate
on single IoT devices when addressing privacy threats; When analysing privacy
issues, key privacy factors such as data reduction and data aggregation
are overlooked; existing studies ignored the sensitivity of the obtained
information; most useful studies did not include a diverse range of users
when assessing privacy problems; no work has been done to discover compliance
difficulties between an IoT application and different privacy rules; and
current research does not place a premium on providing individuals with real-time
privacy notices. Overall, the authors do an extensive literature review
and provide threats to the study's validity, however, this SLR has the
following limitations: the authors only chose papers and not dissertations,
thesis or books and from the selected papers, only the ones written in english
were considered.

KÃ¼htreiber et al. \cite{Kuhtreiber2022survey} evaluate the frameworks and
tools established for developers, specifically in the case of IoT, and find
that current solutions are difficult to use, only successful in limited
scenarios, and insufficient to handle the privacy problems inherent in IoT
development. This study lacks a comprehensive gap review of the chosen
literature, along with research questions (RQs) establishing the significance
of the articles chosen.

Sicari et al. \cite{sicari2015security} examine current research and ongoing
activities that focus on IoT privacy and security solutions. The authors
start by describing the requirements for IoT privacy and security, such
as access control, confidentiality, and authentication. The authors then
conduct a literature study in connection to these three needs. The authors
came to the conclusion that IoT privacy issues have only been partially
examined and that further attention is required. The study does, however, have
some shortcomings: the prior research analysis focuses primarily on security
requirements while ignoring privacy considerations; the authors do not conduct
a thorough gap analysis on the publications examined; and they do not provide
a comprehensive summary of future research topics in the area of IoT privacy
that require more attention.

Lin et al. \cite{LinSurvey} undertake a literature review to identify security
and privacy vulnerabilities in the three IoT architecture layers: network,
perception, and application. The authors describe the first six fundamental
security properties for these tiers as confidentiality, integrity, availability,
identification and authentication, privacy, and trust. Then, the authors
look at a variety of security threats for each of the three stages. The
authors wrap up by giving a succinct summary of many privacy-preserving
data techniques, including the stages of data collection, data aggregation,
and data analysis. The authors do, however, largely focus on the IoT's security
components and consider privacy to be one of the most crucial security aspects,
rather than viewing privacy as a distinct concern. Despite this, the authors
devote a portion of the paper to privacy and the majority of what they address
concerns security.

Yang's study \cite{yang2022overview} examines the literature from the perspective
of IoT phases, such as data collection, transmission, and storage, or in other
words, perception, network and application layers. In these phases the author explores
security protocols at the physical layer, network solutions, and data storage and
sharing approaches. There is a balance of security approaches or solutions and
non-security related approaches discussed, besides the mentioned security protocols,
the author proposes a software-defined network, for the network layer, that enables
a global view of the network which helps to identify malicious traffic patterns. The
author also suggests the use of differential privacy, privacy-preserving data mining,
blockchain or holochain for the application layer. The author presents a high-level
overview of privacy issues and as a result does not offer a comprehensive analysis of
the various approaches or make many suggestions for future study directions.

In Zubaydi et al. \cite{zubaydi2023leveraging}, different works that address the
integration of blockchain technology that aim to tackle various aspects of privacy
are analysed. This SLR has an overview of the IoT architecture, the various blockchain
types and algorithms and IoT challenges. The authors provide an overview of the
chosen papers and present a detailed comparison between them before analysing
the papers using several categories which include generic approaches, healthcare,
smart environments, IoT device gateway, IoT information systems, management systems
and other approaches. The authors also present a considerable number of open issues
that future works should focus on. Because this SLR is about blockchain technology,
the focus is on privacy through security. Overall, this is a very in-depth SLR on the
use of blockchain technology to address privacy issues.

The Khanna and Kaur \cite{khanna2020internet} research paper provides a comprehensive
literature review on the IoT. 194 publications have been examined and categorized
by a particular domain, including independent living, smart grid, smart house,
environment monitoring, healthcare, industrial processing, and smart agriculture.
Following the assessment of the literature, the authors propose the following
unresolved issues: availability, reliability, mobility, performance, data
confidentiality, network management, scalability, interoperability, security,
and privacy. They argue that because IoT is evolving quickly, current research
will be unable to keep up, the data produced by various sensors is extremely
important and needs to be managed and evaluated with extreme care, and extensive
research in this field will make a significant advancement with the goal of
requiring no human intervention.

The research conducted by Tzafestas \cite{tzafestas2018ethics} focuses on
an overview of broad ethical questions, ideas, and theories that can be
applied in the IoT, IoT security, privacy, and trust elements, and the role
of governments. Before discussing the role of governments, a distinction
between ethics and law is made. Governments play an important role in mediating
the future of IoT, especially since many plan to develop smart cities.
Governments should also ensure that IoT products and solutions are used
exclusively for their specified goal. Privacy protection, security, usability,
user experience, trust, and safety have been identified as the primary
ethical problems in IoT. The author concludes that in order to address
sophisticated and tricky unethical and illegal behaviour across the IoT,
more specific IoT legislation and ethics norms for IoT need to be devised
and constantly updated.

Based on Ziegeldorf et al. \cite{ziegeldorf2014privacy} analysis of the literature,
the following are the most prominent privacy concerns in IoT:

\begin{enumerate}
    \item
    The most prominent concern is \textit{identification}, which binds an
    identifier, such as a name and location, with an individual's identity,
    this also enables and aggravates other threats;
    \item
    \textit{Localization and tracking} is the threat of detecting an individual's
    locations through numerous techniques, such as GPS, internet traffic,
    or smartphone location. This threat requires \textit{identification}
    of some kind;
    \item
    In e-commerce, \textit{profiling} is often used for personalization.
    Organizations collect information about individuals in order to deduce
    their interests via association with other profiles and data sources.
    \item
    \textit{Interaction and presentation} allude to the sharing of private
    information with an unintended audience while doing so through a public
    medium. IoT applications often need extensive user interaction, it is
    expected that users of these systems will obtain information via smart
    devices in their immediate surroundings and that users will interface
    with systems in creative, natural ways. However, many of those modes
    of communication and presentation are already available to the broader
    public, making them apparent to anybody around. When personal information
    is transferred between a system and its user, privacy is breached.
    \item
    \textit{Lifecycle transitions} occur when an IoT device is sold, utilized
    by its owner, and eventually disposed of. There may be an expectation
    that the object deletes all information, yet smart devices frequently
    keep massive volumes of data about their own past throughout their entire
    existence. This might contain personal images and videos, which are
    not always erased following ownership transfer.
    \item
    \textit{Inventory attacks} involve unauthorized entry and the acquisition
    of information about the presence and characteristics of personal things.
    Malicious users might use inventory data to profile the property and
    break in.
    \item
    \textit{Linkage} is the process of connecting disparate systems, when
    systems are connecting different data sources, there is a higher danger
    of unauthorized access and data leakage.
\end{enumerate}

\subsection{Proposed Solutions}

\par This chapter presents a variety of solutions separated by themes that arose
from the structured literature research in order to address the disconnect
between privacy ideas across systems and users.

\subsubsection{User Awareness}

Although individuals have a certain expectation of privacy when using IoT
devices, privacy preferences are complex, as noted by \cite{naeini2017privacy},
some individuals value some aspects more than others. There are multiple studies
on privacy in IoT but not many focus on user privacy awareness or
privacy literacy.

Koohang et al. \cite{koohang2022internet} propose a research model with
the goal to examine the relationship between IoT awareness and IoT privacy,
security, and trust, and how this influences IoT continuing usage. To validate
the model, they conducted a study with 299 participants and discovered that
as users increase their awareness of IoT security and privacy threats, their
privacy and privacy knowledge increases, and as users become savvier about
IoT privacy and security, they place more trust in IoT devices, which affects
their continued intention to use these IoT devices. Previous publications
provide support for the proposed model like \cite{tsourela2020internet, knijnenburg2022modern}.
The authors acknowledged the method's shortcomings due to use of a traditional
statement-based method survey in the study and proposed a scenario-based
method approach and a random sample to validate the results, but they did
not elaborate any further on future research directions.

An interactive theatre experience was developed by Skirpan et al. \cite{SkirpanPrivacy}
as a case study of an innovative approach to gather user awareness about their
online behaviour with regard to privacy. This was created to try to prove
that a simulated experience with a credible privacy problem may encourage
people to take action before actually encountering a catastrophe. The plot
of the play consists in a fledgling tech company that unveiled its revolutionary
artificial intelligence (AI) technology while dealing with a company whistleblower and an untimely
zero-day hack on their system. The public is able to interact with the actors
and influence how the story plays out. Audiences and actors were given the
chance to try on roles, behaviors, and opinions that they would not normally
have access to in ordinary life. The authors had interviews and surveys
done after the plays with audience members however they only did interviews
halfway through production and only a small fraction of the audience actually
participated in this data collection, they also noted that after contacting
people months after the interviews that they did not really changed their
behaviour regarding their privacy rights.

\subsubsection{Legislation}

Some papers seek to improve legislation \cite{WEBER2015618, FabianoInternet}
because otherwise, in their view, privacy rights will not be respected if they
are not enforceable legally, they defend that without the express agreement
of the individual concerned, private information obtained by IoT devices
must not be retained or processed in any form, and necessary procedures
must be taken to guarantee that the data collected is not that of an unrelated
individual. Before regulations like the GDPR, CCPA or the Artificial Intelligence
Act, some papers such as Weber \cite{weber2010internet} and Ziegeldorf et al. \cite{ziegeldorf2014privacy}
urged for better regulations to protect personal privacy in IoT systems.
But better protection legislation for individuals would also create opposition
from most companies that want to extract as much private data from their
users without (m)any restrictions in order to increase their profit margins.

The research conducted by Hadzovic et al. \cite{hadzovic2023path} focuses on
present initiatives aimed at establishing an IoT and AI regulatory and legislative
framework in the European Union (EU), as well as its pertinence in developing nations, the authors
choice to focus on EU is due to EU's claim for becoming a global leader \cite{european2021europe}. The authors
identify three steps toward the development of an IoT and AI regulatory and
legislative framework. The first step is to develop a national AI strategy, which
is an important blueprint for increasing AI adoption and should be developed
in accordance with a country's strategic priorities. To that end, a proactive
information and communications technology (ICT) regulator could initiate and encourage
national AI strategy development. The second step in developing a regulatory
framework for IoT and AI involves the consideration of various aspects of IoT
and AI and navigating through many overlapping policy areas to determine the rules.
The legislation must be designed to be future-proof and not restrict technological
development. The ICT regulator, which plays a central role in improving innovation
and developing the electronic communications market, could also play a central role
in the context of IoT and AI. It could serve as a coordinating authority within
an advisory committee before the establishment of a national supervisory authority
for AI. The new regulatory framework should be prepared and assessed using the regulatory
impact assessment (RIA) method to select the best option for the country. And the
third step in the development of a regulatory framework for IoT and AI involves
redefining the role of regulatory authorities. The state must establish a national
supervisory authority, but it is also important to involve civil society, the private
sector, and academia in the process to ensure success. This approach is known as
multistakeholder governance development.

\subsubsection{Privacy through Security}

Sun et al. \cite{SunSecure} design a lightweight communication strategy
for a remote-control system, employing two types of Virtual-Spaces to achieve
the aim of identity announcement and data exchange. They constructed a prototype
system of the scheme and tested it on the Freenet, demonstrating that the
method can effectively resist the influence of flow analysis on communication
anonymity while preserving communication data security.

Motivated by privacy concerns in IoT, Xiong et al. \cite{xiong2018defending}
propose a locally differentially private packet obfuscation mechanism as
a defence against packet-size side-channel assaults in IoT networks. Because
a quantifiable measure of privacy risk was required for preserving privacy,
the authors chose LDP in order to protect individual
IoT devices given that it could mask packet sizes prior to transmission.
The effectiveness of hiding the packet size from various smart home IoT
devices was empirically demonstrated, and it was also clear that this mechanism
works really well when using high bandwidth, but many IoT devices purposefully
use low bandwidth for a variety of reasons, so bandwidth-constrained users
must optimally tune their privacy preferences and trade off privacy with
bandwidth. For further research the authors suggested addressing timing side-channel
attacks.

\subsubsection{Architecture / Frameworks}

Antunes et al. \cite{AntunesFederated} do a SLR on federated learning in
the area of healthcare and make an architecture proposal. The procedure
known as federated learning allows for the distributed training of machine
learning models using remotely hosted datasets without the requirement to
gather and hence jeopardize data. The fundamental goal of the proposed architecture is
to allow healthcare institutions that have access to sensitive medical information
to use it in distributed data analysis and machine learning research while
ensuring patient confidentiality. Because information transmitted among
institutions need confidentiality guarantees for learning model parameters
and analysis results, the architecture can adopt a number of ways based on
a zero-trust security paradigm \cite{ChenSecurity}. Furthermore, the institutions
develop a learning algorithm verification system that can store and disseminate
manifestos, as well as engage in distributed analytic procedures that need
unanimous agreement from all participants. This study also demonstrates
what previous literature implies, that homomorphic encryption and differential
privacy are effective approaches for preventing data breaches without incurring
prohibitively high computing costs.

Opara et al. \cite{opara2022framework} present a system for spotting possible
problems with privacy or security regulations in the early stages of development,
this approach is intended at developers. The paper proposes a domain-specific
ontology for modelling IoT security and privacy policies, a notation for
representing and validating IoT security and privacy policies, a set of
guidelines and rules for detecting IoT policy errors, and a tool for visually
modelling and capturing IoT security policies and discovering policy problems.
Although the framework that is presented is theoretically promising it has
not been tested in a real environment so the effectiveness cannot yet be
measured. The authors also do not compare their proposal with others already
available.

A Privacy by Design (PbD) framework is proposed by Perera et al. \cite{perera2020designing}
to assist software developers in incorporating concerns about data privacy
into the design of IoT applications. The proposed framework consists in
a set of guidelines. The authors conducted two studies, one interview based
in person and the other online, the first study was done with software engineers with
various levels of experience and the second was done with master's students.
The authors note that developers do not have a privacy mindset, meaning
they do not regard privacy as a primary aspect of the application design,
giving more importance to other aspects like functionality or security.
They also mentioned that using this framework must be context-based, since
it may be difficult and prone to over-analysis by engineers. When citing
related works, they state that various PbD frameworks exist, but none are dedicated
to IoT, which is incorrect as evidenced by other works \cite{o2017privacy, cavoukian2016embedding},
with \cite{alkhariji2023semantics, aljeraisy2021privacy} being more recent
examples. It is true, however, that there are relatively few IoT PbD frameworks.

Q. Zhang et al. \cite{zhang2017privacy} propose a double-projection deep
computation model (DPDCM) and a privacy-preserving (PPDPDCM) variant for
big data feature learning. The DPDCM extends the deep computation model
(DCM) by replacing all hidden layers with double-projection layers.
Because the learning algorithm is very time-consuming the authors choose
to leverage cloud computing to increase efficiency by crowdsourcing data
on the cloud, but in big data sets there are a large number of private data
so to solve this problem the authors propose a PPDPDCM by using the BGV
scheme to encrypt the private data. After testing the proposed models,
they conclude that the DPDCM is marginally better than the conventional DCM
with a 3\% to 4\% improvement, but the PPDPDCM performs similar to DPDCM
without a noticeable classification accuracy drop while still preserving
individual's private data.

There exists frameworks like the NIST Cybersecurity Framework \cite{barrett2018framework},
published by the National Institute of Standards and Technology (NIST),
which includes a number of recommendations for reducing organizational
cybersecurity risks. Although this framework is primarily focused on
security, it can also be used to reduce privacy threats.
ISO/IEC 27400:2022 \cite{iso2022cybersecurity}, developed by the International
Organization for Standardization (ISO), is a standard that provides
guidelines on risks, principles and safeguards for the security and
privacy of IoT systems. In addition, ISO has produced about 208 privacy-related
standards \cite{iso2023privacysearch}, some of which have been published
and others of which are currently in development. This is a small number
when compared to the roughly 1305 security-related standards \cite{iso2023securitysearch},
but there may be some overlap between these standards. Even those ISO privacy-based
standards have a good amount of intersection with security related
guidelines.

\subsubsection{Blockchain}

Blockchain is an option to guarantee privacy in IoT because of zero-knowledge
proofs\cite{sun2021survey}, ring signatures \cite{mercer2016privacy} and mixing \cite{stone2021trustless}
among other techniques \cite{zhang2019security}.

A zero-knowledge proof is a cryptographic technique that enables one party
to demonstrate to another that a certain statement is true without disclosing
any information other than the validity of that statement. Completeness, soundness,
and zero-knowledge are the three requirements that must be satisfied by a
zero-knowledge proof method. Completeness states that if a
statement is genuine, an honest party will be convinced of it by another honest
party. Soundness indicates that a nefarious party should only have a small chance
of convincing an honest party that a statement is true. Zero-knowledge states
that the method must only tell one party whether or not the other party is
disclosing the truth.

Ring signatures create a single, recognizable signature that is used to sign
a transaction by combining a number of partial digital signatures from diverse
users. This group, known as the ring, can be chosen at random from the outputs
that other users have made to the blockchain. A ring signature has the security
property that it should be computationally expensive to determine which
of the group's members' keys was used to produce the signature, this is because
it obfuscates the input side of a transaction. A user's anonymity cannot
be taken away from their signature, and any group of users can act as a signing
group automatically.

Mixing is the process of blending possibly traceable digital assets
with others to obscure the original assets' sources. This is frequently done
by pooling source assets from different inputs for a long period and at random
intervals, then spitting them back out to destination addresses. Since they
are all packed together and then delivered at random intervals, it is very
difficult to pinpoint particular assets. Due to the fact that cryptocurrencies
provide a public record of every transaction, mixers have been developed
to improve cryptocurrency privacy. Because of their emphasis on secrecy,
mixers have been used to launder money using cryptocurrency.

R. Zhang et al. \cite{zhang2019security} outline some disadvantages concerning
these techniques. For example, the authors assert that zero-knowledge proofs
are less efficient than other techniques, that ring signatures cannot reveal
the identity of the signer in the event of a dispute, and that centralized
services run an increased risk of leaking users' personal information when
it comes to mixing.

Yu et al. \cite{yu2018blockchain} discuss various implementations of blockchain
that provide privacy through security, based on different categories like
data integrity, data sharing and authentication and access control, most
implementations proposed use a peer-to-peer network based on the Ethereum
platform, with the exception being using any consortium or private blockchain,
consortium meaning a blockchain where consensus is managed by a pre-selected set
of nodes and private meaning access permission and read/write authority are
tightly controlled. The authors use privacy as a proxy for security, they also
do not discuss the weak and strong points of each implementation or make any
comparison, they also do not provide further research questions.

Ali et al. \cite{AliIoT} suggest a software stack that combines peer-to-peer
file sharing with blockchain smart contracts to offer IoT users control
over their data and do away with the necessity for centralized IoT data
management. Blockchain smart contracts are used in the proposed `modular
consortium' architecture to regulate access while establishing responsibility
for both data owners and other parties that users grant access to.

\subsubsection{Privacy Assistants}

There exists a number of privacy assistants in the market. Privacy assistants
have the objective of giving the user flexibility in choosing the preferred
privacy options in available applications, most are used in smartphones,
very few are made for devices in the IoT.

The Carnegie Mellon University CyLab, which is the university's security
and privacy research institute, started developing in 2019 an IoT Infrastructure
that intended to be free of privacy leaks and software covered by their
Secure and Private IoT Initiative 2019, this project would fall under
their main research theme of Trust. In this project they started the design
of a Personalized Privacy Assistant (PPA) \cite{ColnagoInforming}, this
would involve the use of semi-structured interviews with 17 participants
to examine user perceptions of three hypothetical PPA implementations,
each of which is potentially more autonomous, while outlining the advantages
and disadvantages of each implementation. The interviews were divided into
three sections: exploratory, anchoring and the PPA; While the exploratory
phase's purpose was to learn about participants' attitudes and understanding
of IoT, the anchoring phase aimed to normalize participants' basic understanding
of how IoT functions. In order to get people to think about potential privacy
concerns towards the end of the anchoring section, the authors asked participants
about their opinions on data privacy. In the PPA section, it was proposed
the idea of a PPA for IoT as a potential future project. The authors clarified
that the PPA could distinguish between active data requests such as a gadget
asking biometric information from the user's health tracker and passive
data collection such as a smart device with a microphone that could record
people's utterances while they were nearby. The Notification, Recommendation,
and Auto implementations of an IoT PPA were the three that the authors and
attendees discussed. Notification PPAs can determine which adjacent devices
are requesting data and alert users to those devices' presence and requests
so that users can approve or reject each request. Building on notification
PPAs, recommendation PPAs offer advice to individuals on how to share their data
based on their preferences. The user's data sharing decisions would be made
by auto PPAs. This would lessen the cognitive load on individuals but also
take away their ability to influence the process. They found that the participants'
attitudes regarding the various implementations were generally favourable,
although they also voiced worries, which varied depending on the degree
of automation. Given the divergent motivations of participants some desired
increased control, while others wished to avoid being overtaken by notifications
and the lack of agreement regarding the optimal PPA implementation.

After the design phase, the institute implemented a privacy assistant (PA) \cite{FengDesign},
the authors called it IoT Assistant. Because the predominant approach of
``notice and choice'' for data privacy protection, they decided the PA would
also fall into this approach, but because many systems implement notice
as a form of consent, without sometimes offering choices to the end user,
they also wanted this work to provide a conceptual framework that views
user-centred privacy choice as well as a taxonomy for practitioners to
use when designing meaningful privacy choices for their systems. The authors
define meaningful privacy choices as ``the capabilities provided by digital
systems for users to control different data practices over their personal
data'', They extend the notion of privacy choices with five facets: effectiveness
(the opportunity to establish privacy preferences that precisely and completely
match the data collection and use methods that a user is okay with), efficiency
(the capacity to specify these options with the least amount of effort and
time), user awareness (where significant privacy options should be prominently
and clearly communicated to users), comprehensiveness (users should understand
their options, how they affect the gathering and potential use of their
data, as well as what conclusions might be drawn from this data and the
potential repercussions of these conclusions) and neutrality (meaningful
privacy decisions should not be subject to manipulation or bias). The IoT
Assistant offers four privacy settings, giving end users a variety of alternatives
to better suit their varied privacy preferences and as a result, privacy
options are more effective in the IoT environment. The IoT Assistant acts
as a centralized privacy choice platform by implementing various privacy
options, allowing individuals to more effectively govern their data privacy
in IoT. The three IoT system discovery modes that the IoT Assistant supports
are QR codes, push notifications, and location-based map interfaces. These
discovery tools are probably going to make users more aware of the installed
IoT devices and the privacy options they have. Additionally, the united
viewpoint of the integrated notification and option in the the IoT Assistant
gives succinct yet thorough information regarding IoT data practices to
help users better understand the implications of their privacy choices.
Additionally, the authors work to implement the integrated notice and option
in the IoT Assistant without bias or framing, attempting to offer individuals
a neutral space to execute their privacy choices. Although the authors view
the IoT Assistant as a significant step towards ``meaningful privacy options''
in IoT, this assistant still has many problems, such as the fact that it
is still in its early stages of development and that there has not been much
growth given that it was created in 2020 and we are in 2023. Maybe the main
reason this application was not able to be developed further is that the
application itself serves to show the user the data that is already in the
IoT infrastructure that was created before, and as such it is not capable
of identifying new IoT devices without the end users themselves create on
the infrastructure's main webpage \cite{DasPersonalized} a new entry for
the device in question that the user wants to interact with. Another reason
that cripples this application as well as others that seek to provide better
privacy in IoT systems is that many systems do not offer any type of privacy
choices to the end user or to other users that are not the intended end
users, but the devices are still collecting data about.

The IoT infrastructure that was developed \cite{DasPersonalized} is built
on an open, distributed design that allows for the deployment and management
of IoT resources to be carried out by any number of actors. Part of this
infrastructure is the Internet of Things Resource Registry, it is a web
platform that enables resource owners to declare not only the place where
a resource is deployed but also data practices like the reason(s) for a
particular data collecting process, the level of detail in the data being
gathered, retention, the recipients of the data, and more. Additionally,
it discloses any user-configurable privacy settings that might be connected
to a particular resource.

\subsubsection{Other Proposals}

Zhu et al. \cite{ZhuIntegrating} present a hybrid sensor system that safeguards
privacy while also monitoring parking availability. The authors merged IoT
sensing with crowdsensing and enhanced it with privacy-preserving methods.
The authors employed physical hazy filters to mask IoT sensors,
and a cryptographic technique based on cryptographic commitments, zero-knowledge
proofs, and anonymous credentials in crowdsensing. In addition, they used
crowdsourcing to create a machine learning model for parking recognition
in the presence of foggy filters. Their paper included proof-of-concept
prototypes such as a Raspberry Pi system and a mobile app, as well as an
evaluation study of the machine learning model and the effects of crowdsourcing.

Lola et al. \cite{electronics12122589} propose a system that manages IoT
device network communication by having manufacturers declare their device's
data collection intentions while simultaneously allowing IoT users control
over their data privacy and security, and thus providing transparency to
IoT systems. The system's design includes tools for analysing packets sent
by IoT devices and executing network traffic control rules. The goal is
to enable the declaration and verification of IoT device communication
intentions, as well as to govern such communication in order to detect
potential security and privacy violations. This system's limitations include
the fact that it can only handle non-encrypted network traffic, only working
in TCP/IP networks, and end-users' inability to adequately establish
their user policies due to lack of technical knowledge. The authors
suggest using machine learning to improve this system, homomorphic encryption
and/or federated learning could also be used.

IoT sniffers are usually used to detect problems in the networks, they rarely
are used to provide privacy for the users.

The LTEye project \cite{KumarLTE} is an open platform that provides granular
temporal and spatial analytics on the performance of LTE radios without access
to private user data or provider assistance. Despite the presence of multipath,
LTEye uses a revolutionary extension of synthetic aperture radar to communication
signals in order to precisely pinpoint mobile users.

\subsection{Privacy Challenges}

IoT is a composed of a complex web of architectures, applications and technologies.
In terms of architectures, it can be decomposed in three layers: the perception
layer, the network layer and the application layer.

The perception layer, also known as the sensor layer, interacts with physical
objects and components via smart devices (RFID, sensors, actuators, and
so on). Its key objectives are to connect objects to the IoT network and
to monitor, collect, and analyze status information about these things using
deployed smart devices. This layer can often be unreliable, for instance
with autonomous vehicles where they find it hard to read road signs or to
predict if certain objects are inanimate or not, but this unreliability
also brings privacy even though some of the data might be unusable. Noise
can also be added in this layer to provide extra privacy.

In the network layer there are many competing networks like ZigBee, Z-Wave,
Bluetooth Low Energy, LoRa, Wi-fi, etc., this layer is fragmented specially
in regards to wireless networks and that makes it very difficult to create
an IoT architecture that can use various networks and have the various
devices communicate with each other, even though interoperability is seen
as a very important factor in IoT. Some of these networks are open standard
protocols while others are proprietary and use different protocols of communication,
use different frequencies, different ranges and different data rates. When
creating an IoT architecture the designers often think of how to solve
specific problems and use what is best for the current needs, and the way
that IoT is fragmented does not help in providing progress.

The application layer receives data from the network layer and uses it to
execute essential services or operations. This layer, for example, can provide
the storage service to backup incoming data into a database or the analysis
service to analyze received data in order to predict the future state of
physical devices. This layer encompasses a wide range of applications, each
with its own set of requirements. A few examples are smart grids, smart
transportation, and smart cities.

Several major challenges that need to be addressed have been identified by
Qu et al. \cite{Qu2018Privacy}, including the absence of a theoretical foundation,
the need to balance privacy and data utility, and the over-complexity of system
isomerism. Isomerism is a concept borrowed from chemistry, which refers to
molecules that have the same molecular formula, but distinct arrangements of
atoms in space \cite{petrucci2023general}. The design of IoT structures lacks
mathematical foundations and is based on empirical methods, which hinders
the development of IoT. Optimizing IoT performance based solely on human experience
is difficult, as is implementing privacy protection mechanisms without theoretical
guidance. Adversaries can take advantage of this to increase the success rates
of their attacks. Trade-off optimization must be based on scientific theory
and quantitative analysis, but this is complicated by the presence of multiple
parties with dynamic characteristics and diverse requirements. The lack of a
theoretical foundation leads to non-uniform quantitative measurements and introduces
uncertainty into trade-off optimization. The large number of standards and protocols
contributes to the over-complication of system isomerism, which hinders communication
and system integration. Ensuring effective IoT applications without wasting resources
leaves less for privacy protection, but lightweight privacy protection cannot
meet all requirements. Adversaries can also exploit structural information to
launch various attacks.

In the context of big data privacy, Ranjan et al. \cite{perera2015big} point
out several challenges, such as obtaining user consent, giving users absolute
freedom to control their data, full transparency of the data life cycle, anonymity
technology, security throughout the dataflow and stakeholder responsibility.
It is challenging to create technologies that efficiently and effectively solicit
consent from users, given that each user has limited time and technical expertise
to participate in the process, to resolve this the authors suggest
integrating the principles and methods from both human-computer interaction and
cognitive sciences. Data owners should have complete control over their data,
yet current solutions provide restricted user access. Users should be able to
select hardware and software, choose the sort of data they share and grant access
privileges to, as well as be allowed to revoke or modify previous consents.
Service providers should not disable functionality or change membership fees,
to encourage consent. Regarding transparency of the data life cycle, without
clear user approval, service providers shall not use previously gathered data
for any other purpose. Given that technology should offer users anonymity, a
comprehensive architecture for anonymization is necessary to facilitate complete
anonymity in the IoT. This anonymity needs to be guaranteed at various stages,
such as data modelling, storage, routing, communication, analytics, and aggregation.
It is the duty of all stakeholders to secure the infrastructure, the data collecting
and transfer process, and the individuals who use the devices. Security upgrades
should require as little user intervention as possible.
This paper is one of the few that outlines stakeholder responsibility. The
authors identify five major stakeholders: device manufacturers, IoT cloud services
and platform providers, third-party application developers, government and regulatory
bodies, and individual users and non-users. Device manufacturers must be responsible
for including privacy-preserving techniques into their devices, informing users
on the types of data collected by their devices, explain the data processing
processes used, and specify when and how data will be extracted, they must also
provide users with the option to disable hardware components and third-party
developers with programming interfaces. IoT cloud services and platform providers
must be responsible to use common standards, so it allows users to have a choice
of providers and to easily move data between them. Local software and hardware
gateways, such as mobile phones, can encrypt, process, and filter data locally,
minimizing the quantity of data transferred to the cloud and the risk of user
privacy violations. Application developers are responsibility for making sure
their programs are free of malware and for giving users accurate information.
Users must expressly consent before using any features of the app, and they must
have the choice of which features to enable as well as the flexibility to withdraw,
grant, or change their consent at any time. Users should also have full access
to the data collected by IoT devices. Government and regulatory bodies should be
responsible for the enforcement of standardization and legal efforts, while also
allowing interoperability among different IoT solutions, a fair marketplace,
and competition. The authors suggest a governing body similar to the World Wide
Web Consortium for the IoT to oversee standardization and certification processes
and that the IoT certification model should be much broader because it might need
to certify both hardware products and software services.
The majority of IoT systems currently in use are primarily geared toward users,
although some IoT system types can also have an impact on non-users. The authors
suggest using notifications that are similar to closed-circuit television (CCTV) surveillance in public spaces,
but because monitoring and actuation tasks are difficult, it may be required to use
interactive and digital tools to educate non-users.

Except for the papers focusing on the privacy paradox, very few papers discussed
stakeholder responsibility. The ethical side of privacy is also not often examined.
Privacy literacy is, however, somewhat more widely studied, though primarily
outside the context of IoT, similar to the studies on the privacy paradox.
