\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{ {./assets/} }
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\hypersetup{
    colorlinks=false,
}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Empowering Users' Privacy Rights in the Internet of Things\\
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Nelson Vieira}
\IEEEauthorblockA{\textit{Faculdade de Ciências Exatas e da Engenharia} \\
\textit{Universidade da Madeira}\\
Funchal, Portugal \\
2080511@student.uma.pt}
}

\maketitle

\begin{abstract}
Internet of things (IoT) devices are everywhere, since the birth of ubiquitous
computing that human every day life is envisioned containing millions of
devices that control every aspect of our lives. Today we have smart cars,
smart houses, smart cities, wearables among other things that use various
types of devices and various types of networks to communicate. These devices
create new ways of collecting and process personal data from users and
non-users. Most end users are not even aware or have little control over
the information that is being collected by these systems. This work takes
an holistic approach to this problem by first doing a literature review,
then by doing a survey to gather information about the general knowledge
of Portugal's population in this very topic and then, based on the information
gathered, it is proposed a system that gives users information about the
devices that are nearby and how to protect the data that they do not want
to share with these devices, this system is capable of detecting what type
of devices are nearby, what kind of data is collected by these devices,
show privacy choices to the user when it is possible to do so and what can
be done to protect unwanted data from being collected.
\end{abstract}

\begin{IEEEkeywords}
privacy, Internet of Things, ubiquitous computing, privacy assistant
\end{IEEEkeywords}

\section{Introduction}

Privacy as we know it is a somewhat recent concept \cite{vincent2016privacy, moore2017privacy},
before the digital age there was barely any notion of privacy for most
people. For many centuries most people used to reside in small communities
where they were continuously involved in one another's lives. Even more
recent is the idea that privacy is a crucial component of personal security,
in contrast to the undeniable necessity of public security, including the
requirement for guarded walls and closed doors. Long seen as a luxury, privacy
is still usually regarded as a good to have rather than an essential
requirement, even though it is acknowledged as a human right, as present in
article 12 of the Universal Declaration of Human Rights \cite{RooseveltUniversal}:
``No one shall be subjected to arbitrary interference with his privacy,
family, home or correspondence, nor to attacks upon his honour and reputation.
Everyone has the right to the protection of the law against such interference
or attacks''. Privacy can be defined \cite{InternationalWhat, SpiekermannEngineering}
as the right to govern how personal information and data is collected, stored,
and used, it frequently involves handling sensitive information with care, and
as such, organizations must be open and honest about the kind of data they plan
to gather, why they need it, and where and with whom they plan to share it.
Users should have the right to control their shared information.

This definition can cause some confusion with the idea of security \cite{HIVDifference}
and although privacy and security are interconnected, security involves
measures taken to safeguard data from risk, threat or danger, it frequently
alludes to safety. It is the practice of keeping users' personal information
and data safe and preventing unauthorized access to it. The primary contrast
between privacy and security is that the former deals with information personal
to individuals and how they want their data used and maintained, whilst the
latter deals with its protection from possible threats. Security can exist
without privacy, but the opposite is not true. For managing sensitive and
personal data, privacy and computer security are equally crucial.

Concerns about digital privacy have been growing \cite{emami2019exploring, park2022personal, zhang2022peer}
in the last few years, especially after the Anonymous decentralized hacker
group cyber attacks, WikiLeaks and Snowden's leaked top secret documents
from United State's National Security Agency. These concerns can be noted
with the increase of written literature on the subject, when searching for
terms like ``privacy'', ``online privacy'', ``digital privacy'' in Google
Scholar, ACM Digital Library or Science Direct it can be seen that, in the
last 5 years, it returns about 5000000, 650000 and 80000 documents respectively,
including articles, books, conference papers etc.

Most research has focused on the web, while privacy in IoT systems has not
been explored as much. Although IoT devices are becoming more prevalent,
new methods of communicating, gathering, and analyzing data emerge.
Because there is already a substantial quantity of research focusing on web
privacy rather than IoT privacy, it is a lot more fertile ground to explore
the issue of privacy in the context of the Internet of Things.

IoT is a term that first appeared in the 1990s, and it may be linked to Mark
Weiser's paper on ubiquitous computing \cite{weiser1991computer} and the growth
of devices of all sizes that communicate with one another to do various tasks,
making Weiser's dream a reality. These devices are used in various applications,
beginning at home \cite{marikyan2019systematic} with thermostats, fridges,
microwaves, etc, in smart cars \cite{arena2020overview}, in the education
system \cite{al2020survey}, in our clothes and our watches \cite{niknejad2020comprehensive}
and even into outer space \cite{AkyildizInternet}. IoT resources may include
IoT equipment (like smart home assistants and autonomous vehicles), IoT services
(like video analytics services linked to smart cameras and indoor position
tracking systems), or IoT apps (like smart TV remote apps) that track and use
information about us. The first use of the term \textit{Internet of Things} was
in 1999 by Kevin Ashton \cite{KevinThat}, executive director of the Auto-ID
Center of MIT, during a presentation for Procter \& Gamble.


@article{rose2015internet,
  title={The internet of things: An overview},
  author={Rose, Karen and Eldridge, Scott and Chapin, Lyman},
  journal={The internet society (ISOC)},
  volume={80},
  pages={1--50},
  year={2015},
  publisher={Reston, VA}
}

The term “Internet of Things” (IoT) was first used in 1999 by British technology pioneer Kevin Ashton to
describe a system in which objects in the physical world could be connected to the Internet by sensors.12
Ashton coined the term to illustrate the power of connecting Radio-Frequency Identification (RFID) tags13
used in corporate supply chains to the Internet in order to count and track goods without the need for human
intervention. Today, the Internet of Things has become a popular term for describing scenarios in which
Internet connectivity and computing capability extend to a variety of objects, devices, sensors, and everyday
items.
While the term “Internet of Things” is relatively new, the concept of combining computers and networks to
monitor and control devices has been around for decades. By the late 1970s, for example, systems for
remotely monitoring meters on the electrical grid via telephone lines were already in commercial use.14 In the
1990s, advances in wireless technology allowed “machine–to–machine” (M2M) enterprise and industrial
solutions for equipment monitoring and operation to become widespread. Many of these early M2M
solutions, however, were based on closed purpose–built networks and proprietary or industry–specific
standards,15 rather than on Internet Protocol (IP)–based networks and Internet standards.
Using IP to connect devices other than computers to the Internet is not a new idea. The first Internet
“device”—an IP–enabled toaster that could be turned on and off over the Internet—was featured at an
Internet conference in 1990.16 Over the next several years, other “things” were IP–enabled, including a soda
machine17 at Carnegie Mellon University in the US and a coffee pot18 in the Trojan Room at the University of
Cambridge in the UK (which remained Internet–connected until 2001). From these whimsical beginnings, a
robust field of research and development into “smart object networking”19 helped create the foundation for
today’s Internet of Things.











The Internet of Things can be defined as: ``An open and comprehensive network
of intelligent objects that have the capacity to auto-organize, share information,
data and resources, reacting and acting in face of situations and changes in
the environment'' \cite{madakam2015internet}.

IoT is one of the fastest growing technologies \cite{MohammadState}, it is
predicted that it will grow into the trillions of devices by 2030 \cite{SarawiInternet},
and with this expansion new security vulnerabilities and data gathering dangers
appear, the lack of security in these devices makes them ideal targets for privacy
violations and inadequate customer disclosure of device capabilities and data
practices aggravates privacy and security issues.

Privacy in IoT systems in not seen as a crucial factor in development \cite{alhirabi2021security}.
Specific standards for privacy options have been imposed by data privacy regulations
including the General Data Protection Regulation (GDPR) and California Consumer
Privacy Act (CCPA), but even these regulations have been criticized \cite{peloquin2020disruptive, gladis2022weaponizing, gentile2022deficient, green2022flaws, byun2019privacy}.

\section{Literature review}

The literature selected for discussion further down in this section is derived
from the following research questions:

\textbf{RQ1:} What approaches are being considered for privacy issues in IoT
in the literature currently available?

\textbf{RQ2:} What IoT-related tools are accessible to users to empower them to
protect their privacy rights?

\textbf{RQ3:} What issues are prevalent in IoT that make it difficult to address
privacy and security problems?

Some relevant topics are discussed before analyzing the proposals that the
literature regards as solutions to resolve privacy issues in IoT.

\subsection{Privacy Paradox}

The use of a variety of digital devices have numerous advantages, but they also
bring with them the ubiquity of data capturing equipment, therefore, it is
understandable why the majority of online users have serious concerns about
the privacy of their personal data. However, the opinions expressed are starkly
at odds with the reality, according to Thomson et al. \cite{DarrenState} report
on the state of privacy, that just one in four European users read the terms
and conditions in their entirety prior to making an online purchase or
subscribing to a service, 59\% admitted to only quickly scanning the terms and
conditions before completing a purchase, while 14\% admitted to never reading
them at all, 30\% of the respondents would even swap their email address to win
a reward, or entry into a raffle, while 17\% would do so to get an app and 30\%
would do it for money.

This is what is called a privacy paradox, there have been multiple papers
written on this subject \cite{solove2021myth, WilliamsPrivacy, lee2021investigating, goad2021privacy, gerber2018explaining},
some papers attempt a theoretical explanation while others attempt an empirical
one. There has been very different interpretations or explanations of this
paradox, a few papers \cite{wilson2012unpacking, warshaw2015can, lee2015privacy}
apply the theoretical concept of the \textit{homo economicus} \cite{zak2008moral},
which is the representation of people as beings who constantly act in a way
that is logical and self-interested, not worrying about morality or ethics,
and who do so to the best of their ability, to the context of privacy. Different
cognitive biases and heuristics can influence how consumers make decisions,
according to several studies on consumer choice behavior \cite{acquisti2007can, knijnenburg2013dimensionality, wakefield2013influence, flender2012type}.
According to several articles \cite{dienlin2015privacy, baek2014solving},
this paradox might be explained by the fact that some people have genuinely
experienced online privacy assaults and that most privacy views are therefore
based on heuristics or secondhand accounts. Taddicken's study \cite{taddicken2014privacy}
argues that peer pressure is the reason people have this contradictory behavior,
Norberg et al. \cite{norberg2007privacy} explains this paradox by suggesting
that while perceived risk affects reported attitudes and behavioral intentions,
trust has a direct impact on privacy behavior, while others \cite{flender2012type, kokolakis2017privacy}
rely on quantum theory. Brandimarte et al. \cite{brandimarte2013misplaced}
have explored the idea that when it comes to their data privacy, users have an \textit{illusion of control}.

This paradox has been proven to be vitiated by a number of empirical studies \cite{dienlin2015privacy, xie2019consumers, SCHWAIG20131, sannon2018privacy},
online privacy practices are founded on separate privacy mindsets and so they
are not inherently paradoxical.

\subsection{Literature overview}

There have been a number of systematic literature reviews (SLR) \cite{Gupta2022Privacy, Kuhtreiber2022survey, sicari2015security, LinSurvey}
and systematic mapping reviews \cite{porras2018security, ahmed2019aspects}
done to study privacy and security issues in IoT.

In Gupta and Ghanavati's \cite{Gupta2022Privacy} SLR, the authors review papers with
methodologies and techniques that identify privacy risks or notify users about these
risks. They divide the literature into the following categories: `Ontological Modeling
and Semantic-based Approaches', `Data-Driven Approaches', `Source Code Analysis-based
Approaches', `User Studies and Survey-based Approaches', `Blockchain-based Approaches'
and `Architectural and Framework-based Approaches'. They then examine current literature
on these three prerequisites. The findings show that: most works concentrate on single
IoT devices when addressing privacy threats; When analyzing privacy issues, key
privacy factors such as data reduction and data aggregation are overlooked; existing
studies ignored the sensitivity of the obtained information; most useful studies
did not include a diverse range of users when assessing privacy problems; no work
has been done to discover compliance difficulties between an IoT application and
different privacy rules; and current research does not place a premium on providing
consumers with real-time privacy notices. However, this SLR has the following
limitations: the authors only chose articles and not thesis or books and from the
selected papers, only the ones written in english were considered.

% Kühtreiber et al. \cite{Kuhtreiber2022survey} conduct a survey of the frameworks
% and tools created for developers, particularly in the case of IoT, and they conclude
% that present solutions are difficult to use, only effective in specific circumstances,
% and insufficient to address the privacy problems inherent in IoT development. This
% study lacks a comprehensive gap analysis of the chosen literature and it does not
% specify the research questions that define the relevancy of the selected papers.

% Kühtreiber et al. \cite{Kuhtreiber2022survey} evaluate the frameworks and tools
% established for developers, notably in the case of IoT, and find that current
% solutions are difficult to use, only successful in limited contexts, and insufficient
% to handle the privacy issues inherent in IoT development. This study lacks a detailed
% gap analysis of the selected literature, as well as the research questions that
% characterize the relevance of the selected publications.

Kühtreiber et al. \cite{Kuhtreiber2022survey} evaluate the frameworks and tools
established for developers, specifically in the case of IoT, and find that
current solutions are difficult to use, only successful in limited scenarios,
and insufficient to handle the privacy problems inherent in IoT development.
This study lacks a comprehensive gap review of the chosen literature, along with
research questions establishing the significance of the articles chosen.

% Sicari et al. \cite{sicari2015security} conduct an analysis of recent studies and
% active initiatives that emphasize IoT privacy and security solutions. They begin
% by outlining the needs for IoT privacy and security, including access control,
% confidentiality, and authentication. They then review the literature that already
% exists in relation to these three needs. They concluded that IoT privacy concerns
% are only partially investigated and need further attention. The study, however,
% has its shortcomings, the analysis of prior research focuses primarily on security
% needs and leaves out privacy considerations, they don't perform a thorough gap
% analysis on the publications that were examined, and they don't provide a thorough
% summary of the future research topics in the field of IoT privacy that need more
% attention.

% Sicari et al. \cite{sicari2015security} examine current research and ongoing activities
% that focus on IoT privacy and security solutions. They start by stating the
% requirements for IoT privacy and security, such as access control, confidentiality,
% and authentication. They next go over the existing literature in connection to these
% three needs. They came to the conclusion that IoT privacy risks have only been
% partially examined and require more attention. The study, however, has flaws: the
% analysis of prior research focuses primarily on security needs and ignores privacy
% considerations; they do not perform a thorough gap analysis on the publications
% examined; and they do not provide a comprehensive summary of future research topics
% in the field of IoT privacy that require more attention.

Sicari et al. \cite{sicari2015security} examine current research and ongoing activities
that focus on IoT privacy and security solutions. The authors start by describing
the requirements for IoT privacy and security, such as access control, confidentiality,
and authentication. The authors then conduct a literature study in connection to
these three needs. The authors came to the conclusion that IoT privacy issues have
only been partially examined and that further attention is required. The study,
however, has flaws: the prior research analysis focuses primarily on security needs
while ignoring privacy considerations; the authors do not conduct a thorough gap
analysis on the publications examined; and they do not provide a comprehensive
summary of future research topics in the field of IoT privacy that require more
attention.

% Lin et al. \cite{LinSurvey} undertake a literature review to find security and
% privacy concerns in the network layer, perception layer, and application layer,
% the three IoT architectural layers. Confidentiality, integrity, availability,
% identity and authentication, privacy, and trust are the first six essential security
% properties that the authors list for these levels. Then, for each of the three
% tiers, they examine a number of security attacks. They conclude by providing a
% brief overview of a number of privacy-preserving data procedures, including data
% gathering, data aggregations, and data analysis stages. They do, however, primarily
% focus on the security elements of the IoT and see privacy as one of the most important
% security characteristics, as was already established. Additionally, a thorough gap
% analysis to determine the shortcomings of the previous works is not done in the
% research.

Lin et al. \cite{LinSurvey} undertake a literature review to identify security and
privacy vulnerabilities in the three IoT architecture layers: network, perception,
and application. The authors describe the first six fundamental security properties
for these tiers as confidentiality, integrity, availability, identification and
authentication, privacy, and trust. Then, the authors look at a variety of security
threats for each of the three stages. The authors wrap up by giving a succinct
summary of many privacy-preserving data techniques, including the stages of data
collection, data aggregation, and data analysis. The authors do, however, largely
focus on the IoT's security components and, as was already said, consider privacy
to be one of the most crucial security aspects, rather than viewing privacy as a
distinct concern. Furthermore, the research does not conduct a thorough gap analysis
to discover the weaknesses of prior efforts.

% Lin et al. \cite{LinSurvey} conduct a literature research to identify security and
% privacy issues in IoT architecture's three levels: network, perception, and
% application. The authors describe the first six fundamental security features
% for these tiers as confidentiality, integrity, availability, identification and
% authentication, privacy, and trust. They next investigate a variety of security
% concerns for each of the three processes. The authors end by offering a succinct
% review of a number of privacy-preserving data methodologies, including data
% collecting, aggregation, and analysis. They do, however, focus mostly on IoT
% security features, and, as previously said, they see privacy as one of the most
% essential security issues. Furthermore, the study did not undertake a detailed gap
% analysis to identify the shortcomings of previous studies.

\subsection{Differential Privacy}

The notion of differential privacy, according to Michael Kearns \cite{kearns2019ethical}, is based on
three important principles. The first being that ``differential privacy requires
that adding or removing the data record of a single individual not change the
probability of any outcome by much''. The second principle being that ``no outside
observer can learn very much about any individual because of that person's
specific data''. The third important principle being that ``for every individual
in the dataset, and for any observer no matter what their initial beliefs about
the world were, after observing the output of a differentially private computation,
their posterior belief about anything is close to what it would have been had
they observed the output of the same computation run without the individual's
data''.

Differential privacy has the potential to significantly increase individual
privacy protection, by purposefully adding noise into a dataset, it gives
plausible deniability to any individual who may have had their data exploited
while still being able to calculate statistics with relatively high precision.
Although algorithms that deal with notions of fairness, ethics, and privacy
are hard to implement because of the subjectivity of these concepts, and differential
privacy algorithms are no different, they can still help in regards to addressing
technology's inherent moral quandaries.

% Differential privacy is an important component of machine learning algorithms
% and massive datasets, and it has the potential to significantly increase
% individual privacy protection. By purposefully adding noise into a dataset,
% we may give plausible deniability to any individual who may have had their
% data exploited to damage them while still being able to calculate critical
% statistics with high precision. While differential privacy has obvious drawbacks,
% as do many algorithms used to address societal issues, the use of automation and
% machine learning to address subjective notions of fairness, ethics, and privacy
% is a tremendously groundbreaking discovery in the world of computation, and is
% moving us as a society closer to addressing technology's inherent moral quandaries.
% Differential privacy is a fundamental aspect of machine learning algorithms and huge datasets that may substantially increase individual privacy protection. We can provide plausible deniability to every individual who may have their data used to harm them by purposely injecting noise into a dataset, while yet being able to calculate required statistics with high accuracy. While differential privacy, like many algorithms used to address societal issues, has obvious drawbacks, the use of automation and machine learning to address subjective notions of fairness, ethics, and privacy is an incredibly groundbreaking discovery in the world of computation, and is moving us as a society closer to addressing the inherent moral dilemmas of technology.
% Differential privacy is a crucial component of machine learning algorithms and massive datasets that has the potential to significantly boost individual privacy protection. By purposefully inserting noise into a dataset, we can give plausible deniability to any individual who may have their data exploited to harm them while still being able to calculate needed statistics with high precision. While differential privacy has obvious drawbacks, as do many algorithms used to address societal issues, the use of automation and machine learning to address subjective notions of fairness, ethics, and privacy is an incredibly groundbreaking discovery in the world of computation, and is moving us as a society closer to addressing the inherent moral dilemmas of technology.
% Differential privacy is a critical component of machine learning algorithms and large datasets, with the potential to greatly improve individual privacy protection. We can provide plausible deniability to any individual who may have their data misused to harm them while still being able to calculate essential statistics with high accuracy by purposely injecting noise into a dataset. While differential privacy has obvious drawbacks, as do many algorithms used to address societal issues, the use of automation and machine learning to address subjective notions of fairness, ethics, and privacy is an incredibly groundbreaking discovery in the world of computation, and is moving us as a society closer to addressing technology's inherent moral dilemmas.

There exist other algorithms that aim to preserve privacy in the same way as
differential privacy such as Google's box blurring algorithm \cite{FromeLarge} that is
used in the Google Map's street view, Microsoft's Visor \cite{poddar2020visor} which is a video-analytics-as-a-service
tool and Shokri and Shmatikov's \cite{ShokriPrivacy} system for collaborative deep learning,
however, in general, these algorithms struggle with high computational cost,
internal attacks, or non-provable privacy.

% Zhao et al. \cite{ZhaoSurvey} conduct a SLR on differential privacy
% for unstructured data. The authors present differential privacy methods
% for sensitive content in image, audio, video and text data, they compare
% the various methods and do an utility analysis for each method showing
% the pros and cons of each, the utility loss is measured between the real
% data and its obfuscated version in the experimental evaluations. They
% conclude that differential privacy and its variants have provided
% rigorous privacy guarantees for unstructured data against adversaries
% with arbitrary background information. They also provide possible future
% research topics that have yet to be explored.

Zhao et al. \cite{ZhaoSurvey} conduct a SLR on differential privacy
for unstructured data. The authors present differential privacy methods
for sensitive content in image, audio, video, and text data. They compare
the various methods and perform utility analyses for each method,
highlighting the benefits and drawbacks of each, the utility loss
is measured in experimental evaluations between the actual data and its
obfuscated variant. They come to the conclusion that differential privacy
as well as its variations give stringent privacy protections for unstructured
data against attackers with unpredictable background knowledge. They also
suggest potential future study subjects that have yet to be investigated.

% Privacy concerns arise with the millions of real-world unstructured data including
% images, audios, videos, and texts. Differential privacy has emerged as a de facto
% standard to protect a wide range of data types.
% This article has presented
% differential privacy methods for sensitive content in these unstructured
% data. We have identified that unstructured data are obfuscated based on
% appropriate vector representations, specific privacy models, and vector
% reconstructions. We have discussed possible challenges faced with these
% privacy methods. We have provided an overview of privacy guarantees and
% utility losses in these methods and possible approaches to improve data utility.

% DP and its variants have provided rigorous privacy guarantees for
% unstructured data against adversaries with arbitrary background information.
% While privacy loss has been quantified with the ε value, the utility
% loss is measured between the real data and its obfuscated version in
% the experimental evaluations. Different utility loss metrics are
% adopted to quantify utility losses in different data types. Besides,
% it has been shown that other privacy-protecting methods for unstructured
% data are commonly vulnerable to machine-learning-model-based attacks.
% It is crucial to identify whether DP methods can do away with these
% AI attacks.

\subsection{Proposed solutions}

\subsubsection{Creating new ways for user awareness}

There has been some work done to determine the users awareness of their actions
online regarding their privacy. Skirpan et al. \cite{SkirpanPrivacy} built an
interactive theatre experience, this was created to try to prove that a simulated
experience with a credible privacy problem may encourage people to take action
before actually encountering a catastrophe. The plot of the play consist in a
fledgling tech company that unveiled its revolutionary AI technology while
dealing with a company whistleblower and an untimely zero-day hack on their system.
The public is able to interact with the actors and influence how the story plays
out. Audiences and actors were given the chance to try on roles, behaviors, and
opinions that they would not normally have access to in ordinary life. The authors
had interviews and surveys done after the plays with audience members however they
only did interviews halfway through production and only a small fraction of the
audience actually participated in this data collection, they also noted that
after contacting people months after the interviews that they did not really
changed their behaviour regarding their privacy rights.
% Skirpan et al. \cite{SkirpanPrivacy} built
% an interactive theatre experience, this was proposed in order to expose privacy
% violations in companies, particularly in the capitalistic world where profit
% is prioritized above all else. In this experiment, the public is able to interact
% with the actors and influence the story of the play; there are various endings
% depending on the public's choices throughout the play; in some endings, the
% company would bury the corruption that was going on; in another ending, a
% team of hackers is abducted. Following the performance, the experiment's crew
% would speak with public members about their experiences and explain
% what it was all about, as well as the members' everyday practices with their data.

% An interactive theatre experience \cite{ColnagoInforming}
% was proposed in order to expose privacy malpractices in companies,
% specially in the capitalistic world where profit is prioritize
% above all else, in this experiment the public is able to interact with
% the actors and influence the story of the play, there are various endings depending
% on the public's choices throughout the play, some endings the company would
% bury the corruption that was going on, in another ending a team of hackers
% is able to expose the company's practices to the world. After the play
% the team responsible for the experiment would talk with public members about
% what they experienced and discussed what was it about and the members
% practices with their data on their daily life. After some months
% the team would talk again with the members that were present in the play and
% talk about any changes they have done in the meantime, most said they did
% not change their behavior, one member said that it took more care of The
% information that made available online because it had a bad experience before
% where some private data was exposed that should have not been exposed. All in
% all the experiment did not prove to be a success in changing people's behavior.

\subsubsection{Legislation}

Some papers seek to improve legislation \cite{WEBER2015618, FabianoInternet}
because otherwise, in their view, privacy rights won't be respected if they
are not enforceable legally, they defend that without the express agreement
of the individual concerned, private information obtained by IoT devices must
not be retained or processed in any form, and necessary procedures must be
taken to guarantee that the data collected is not that of an unrelated individual.
But better protection laws for the user would also create opposition from
most companies that want to extract as much private data from their users
without (m)any restrictions in order to increase their profit margins.

\subsubsection{Privacy through security}

Sun et al. \cite{SunSecure} design a lightweight communication strategy for
a remote-control system, employing two types of Virtual-Spaces to achieve the
aim of identity announcement and data exchange. They constructed a prototype
system of the scheme and tested it on the Freenet, demonstrating that the method
can effectively resist the influence of flow analysis on communication anonymity
while preserving communication data security.

\subsubsection{Architecture / Framework Proposals}

Antunes et al. \cite{AntunesFederated} do a SLR on federated learning in the area
of healthcare and make an architecture proposal. The technique known as federated
learning allows for the distributed training of machine learning models using
remotely hosted datasets without the requirement for data amplification. The
fundamental goal of the proposed architecture is to allow healthcare institutions
that have access to sensitive medical information to use it in distributed data
analysis and machine learning research while ensuring patient confidentiality.
Because information transmitted among institutions need confidentiality guarantees
for learning model parameters and analysis results, the architecture can adopt a
number of ways based on a zero-trust security paradigm \cite{ChenSecurity}.
Furthermore, the institutions develop a learning algorithm verification system
that can store and disseminate manifestos, as well as engage in distributed
analytic procedures that need unanimous agreement from all participants. This
study also demonstrates that previous literature implies that homomorphic
encryption and differential privacy are effective approaches for preventing
data breaches without incurring prohibitively high computing costs.
% The proposed architecture's main goal is to enable healthcare institutions with access
% to private medical datasets to employ them in distributed data analysis and machine
% learning studies without compromising patient confidentiality, as the information that
% is shared among institutions requires confidentiality guarantees for learning model
% parameters and analyses results, the architecture can implement different methodologies
% based on a zero-trust security model [67]. The institutions also implement a learning
% algorithm verification, which can store and provide manifestos and participate in
% distributed analysis mechanisms requiring consensus among all participants. This study
% also finds that Current literature shows that homomorphic encryption and differential
% privacy are techniques with substantial results to avoid data leaks without prohibitive
% computational costs [98].

Opara et al. \cite{opara2022framework} present a system for spotting possible
problems with privacy or security regulations in the early stages of development,
this approach is intended at developers. The paper proposes a domain-specific
ontology for modeling IoT security and privacy policies, a notation for
representing and validating IoT security and privacy policies, a set of
guidelines and rules for detecting IoT policy errors, and a tool for visually
modeling and capturing IoT security policies and discovering policy problems.
Although the framework that is presented is theoretically promising it has not
been tested in a real environment so the effectiveness can't yet be measured.
The authors also don't compare their proposal with others already available.

% Opara et al. \cite{opara2022framework} propose a framework for detecting potential
% problems with privacy or security policies from the early pre-production stage,
% this framework is aimed at developers. This paper proposes a domain-specific
% ontology for modeling IoT security and privacy policies, a notation for
% representing IoT security and privacy policies and validating the policies
% for inconsistencies, conflicts, ambiguities, and incompleteness, a set of
% guidelines and rules for detecting IoT policy errors, and a tool for
% visually modeling and capturing IoT security policies and discovering
% policy problems.

% This paper's main contributions are: a domain-specific ontology for modeling
% IoT security and privacy policies, a notation for representing IoT security
% and privacy policies and validating the policies for inconsistencies, conflicts,
% ambiguities, and incompleteness, a set of guidelines and rules for
% detecting IoT policy errors, and a tool for visually modeling and capturing
% IoT security policies and discovering problems with the policies.

% The key contributions of this paper include (1) A domain-specific ontology for modeling IoT security and privacy policies, (2) a notation for representing IoT security and privacy policies and validating the policies for inconsistencies, conflicts, ambiguities, and incompleteness, (3) a set of guidelines and rules for detecting IoT policy errors, and 4) a tool for visually modeling and capturing IoT security policies and discovering problems with the policies.

\subsubsection{Blockchain}

Blockchain proposals that try to resolve privacy issues in IoT.

\subsubsection{Other?}

% Zhu et al. \cite{ZhuIntegrating} explores an integrated paradigm called ``hybrid sensing''
% where users interact with a crowdsensing server via a privacy-preserving protocol
% to preserve their anonymity.

Zhu et al. \cite{ZhuIntegrating} present a hybrid sensor system that safeguards
privacy while also monitoring parking availability. The authors merged IoT
sensing with crowdsensing and enhanced it with privacy-preserving methods.
The authors employed physical hazy filters to mask IoT sensors in IoT sensing,
and a cryptographic technique based on cryptographic commitments, zero-knowledge
proofs, and anonymous credentials in crowdsensing. In addition, they used crowdsourcing
to create a machine learning model for parking recognition in the
presence of foggy filters. Their paper included proof-of-concept prototypes
such as a Raspberry Pi system and a mobile app, as well as an evaluation study
of the machine learning model and the effects of crowdsourcing.

% Zhu et al. describe a privacy-preserving hybrid sensor system with a smart parking availability monitoring application. We combined IoT sensing with crowdsensing and augmented it using privacy-protecting approaches. In particular, in IoT sensing, we concealed IoT sensors using physical hazy filters, and in crowdsensing, we used a cryptographic approach based on cryptographic commitments, zero-knowledge proofs, and anonymous credentials. We also used crowdsourcing to build a machine learning model for parking detection under hazy filters. The paper showed proof-of-concept prototypes, including a Raspberry Pi system and a mobile app, as well as an assessment study of the machine learning model and the impacts of crowdsourcing.

% This study offers a hybrid sensor system that protects privacy while also monitoring parking availability.
% We merged IoT sensing with crowdsensing and enhanced it using privacy-preserving techniques.
% We employed physical hazy filters to disguise IoT sensors in IoT sensing, and a cryptographic technique based on cryptographic commitments, zero-knowledge proofs, and anonymous credentials in crowdsensing.
% We also used crowdsourcing to create a machine learning model for parking recognition in the presence of hazy filters.
% The report included proof-of-concept prototypes such as a Raspberry Pi system and a mobile app, as well as an evaluation study of the machine learning model and the effects of crowdsourcing.

% Zhu et al. propose a hybrid sensor system that both protects privacy and monitors parking availability. We combined IoT sensing with crowdsensing and improved it with privacy-preserving approaches. In IoT sensing, we used physical hazy filters to hide IoT sensors, and in crowdsensing, we used a cryptographic approach based on cryptographic commitments, zero-knowledge proofs, and anonymous credentials. Crowdsourcing was also utilized to develop a machine learning model for parking detection in the presence of foggy filters. Proof-of-concept prototypes such as a Raspberry Pi system and a mobile app were included in the paper, as well as an assessment study of the machine learning model and the impacts of crowdsourcing.

% Zhu et al. offer a hybrid sensor system that safeguards privacy while also monitoring parking availability. The researchers merged IoT sensing with crowdsensing and enhanced it with privacy-preserving methods. They employed physical hazy filters to mask IoT sensors in IoT sensing, and a cryptographic technique based on cryptographic commitments, zero-knowledge proofs, and anonymous credentials in crowdsensing. In addition, crowdsourcing was used to create a machine learning model for parking recognition in the presence of foggy filters. The report included proof-of-concept prototypes such as a Raspberry Pi system and a mobile app, as well as an evaluation study of the machine learning model and the effects of crowdsourcing.

\subsubsection{Privacy Assistants}

There exists a number of privacy assistants in the market. Privacy assistants
have the objective of giving the user flexibility in choosing the preferred
privacy options in available applications, most are used in smartphones,
very few are made for devices in the internet of things.

The Carnegie Mellon University CyLab, which is the university's security
and privacy research institute, started developing in 2019 an IoT Infrastructure
that intended to be free of privacy leaks and software covered by their
Secure and Private IoT Initiative 2019, this project would fall under
their main research theme of Trust. In this project they started the design
of a Personalized Privacy Assistant (PPA) \cite{ColnagoInforming}, this
would involve the use of semi-structured interviews with 17 participants
to examine user perceptions of three hypothetical PPA implementations,
each of which is potentially more autonomous, while outlining the advantages
and disadvantages of each implementation. The interviews were divided into
three sections: exploratory, anchoring and the PPA; While the exploratory
phase's purpose was to learn about participants' attitudes and understanding
of IoT, the anchoring phase aimed to normalize participants' basic understanding
of how IoT functions. In order to get people to think about potential privacy
concerns towards the end of the anchoring section, the authors asked participants
about their opinions on data privacy. In the PPA section, it was proposed
the idea of a PPA for IoT as a potential future project. The authors clarified
that the PPA could distinguish between active data requests such as a gadget
asking biometric information from the user's health tracker and passive data
collection such as a smart device with a microphone that could record people's
utterances while they were nearby. The Notification, Recommendation, and Auto
implementations of an IoT PPA were the three that the authors and attendees
discussed. Notification PPAs can determine which adjacent devices are requesting
data and alert users to those devices' presence and requests so that users
can approve or reject each request. Building on notification PPAs, recommendation
PPAs offer consumers advice on how to share their data based on their preferences.
The user's data sharing decisions would be made by auto PPAs. This would lessen
the cognitive load on consumers but also take away their ability to influence
the process. They found that The participants' attitudes regarding the various
implementations were generally favorable, although they also voiced worries,
which varied depending on the degree of automation. Given the divergent
motivations of participants some desired increased control, while others
wished to avoid being overtaken by notifications and the lack of agreement
regarding the optimal PPA implementation.

After the design phase, the institute implemented a privacy assistant (PA) \cite{FengDesign},
the authors called it IoT Assistant. Because the predominant approach of
"Notice and choice" for data privacy protection, they decided the PA would
also fall into this approach, but because many systems implement notice as
a form of consent, without sometimes offering choices to the end user, they
also wanted this work to provide a conceptual framework that views user-centered
privacy choice as well as a taxonomy for practitioners to use when designing
meaningful privacy choices for their systems. The authors define meaningful
privacy choices as ``the capabilities provided by digital systems for users to
control different data practices over their personal data'', They extend the
notion of privacy choices with five facets: effectiveness (the opportunity
to establish privacy preferences that precisely and completely match the data
collection and use methods that a user is okay with), efficiency (the capacity
to specify these options with the least amount of effort and time), user awareness
(where significant privacy options should be prominently and clearly communicated
to users), comprehensiveness (users should understand their options, how they
affect the gathering and potential use of their data, as well as what conclusions
might be drawn from this data and the potential repercussions of these conclusions)
and neutrality (meaningful privacy decisions should not be subject to manipulation
or bias). The IoT Assistant offers four privacy settings, giving end users a
variety of alternatives to better suit their varied privacy preferences and
as a result, privacy options are more effective in the IoT environment. The
IoT Assistant acts as a centralized privacy choice platform by implementing
various privacy options, allowing consumers to more effectively govern their
data privacy in IoT. The three IoT system discovery modes that the IoT Assistant
supports are QR codes, push notifications, and location-based map interfaces.
These discovery tools are probably going to make users more aware of the installed
IoT devices and the privacy options they have. Additionally, the united viewpoint
of the integrated notification and option in the the IoT Assistant gives succinct
yet thorough information regarding IoT data practices to help users better
understand the implications of their privacy choices. Additionally, the authors
work to implement the integrated notice and option in the IoT Assistant without
bias or framing, attempting to offer consumers a neutral space to execute their
privacy choices. Although the authors consider the IoT Assistant to be a significant
step towards ``meaningful privacy options'' in IoT, this assistant still has many
issues such as this application is still in the early stages of it's existence,
and because this was created in 2020 and we are in 2022 there was not much growth.
Maybe the main reason this application was not able to be developed further is
that the application itself serves to show the user the data that is already in
the IoT infrastructure that was created before, and as such it is not capable of
identifying new IoT devices without the end users themselves create on the
infrastructure's main webpage \cite{DasPersonalized} a new entry for the device
in question that the user wants to interact with. Another reason that cripples this
application and many like it that want to provide better privacy in IoT systems is
that many systems don't offer any type of privacy choices to the end user or to
other users that are not the intended end users but the devices are still collecting
data about.

The IoT infrastructure that was developed \cite{DasPersonalized} is built on an open,
distributed design that allows for the deployment and management of IoT resources
to be carried out by any number of actors. Part of this infrastructure is the
Internet of Things Resource Registry, it is a web platform that enables resource
owners to declare not only the place where a resource is deployed but also data
practices like the reason(s) for a particular data collecting process, the level of
detail in the data being gathered, retention, the recipients of the data, and more.
Additionally, it discloses any user-configurable privacy settings that might be
connected to a particular resource.

\subsubsection{Sniffers}

IoT sniffers are usually used to detect problems in the networks, they rarely
are used to provide privacy for the users.
The LTEye project \cite{KumarLTE} is an open platform that provides
granular temporal and spatial analytics on the performance of LTE radios without
access to private user data or provider assistance. Despite the presence of multipath,
LTEye uses a revolutionary extension of synthetic aperture radar to communication
signals in order to precisely pinpoint mobile users.
% \subsection{User Awareness}\label{AA}
% There has been some work done to determine the users awareness of
% their actions online in regard to privacy. An interactive theatre experience \cite{ColnagoInforming}
% was proposed in order to expose privacy malpractices in companies,
% specially in the capitalistic world where profit is prioritize
% above all else, in this experiment the public is able to interact with
% the actors and influence the story of the play, there are various endings depending
% on the public's choices throughout the play, some endings the company would
% bury the corruption that was going on, in another ending a team of hackers
% is able to expose the company's practices to the world. After the play
% the team responsible for the experiment would talk with public members about
% what they experienced and discussed what was it about and the members
% practices with their data on their daily life. After some months
% the team would talk again with the members that were present in the play and
% talk about any changes they have done in the meantime, most said they did
% not change their behavior, one member said that it took more care of The
% information that made available online because it had a bad experience before
% where some private data was exposed that should have not been exposed. All in
% all the experiment did not prove to be a success in changing people's behavior.











% TODO: SUMMARIZE

% The purpose of a privacy notice is to make a system’s users
% or a company’s customers aware of data practices involving
% personal information. Internal practices with regard to the
% collection, processing, retention, and sharing of personal in-
% formation should be transparent to users. The privacy notice
% acts as a public announcement of those practices. Privacy
% notices can take different shapes and leverage different chan-
% nels, ranging from a privacy policy document posted on a
% ∗Rebecca Balebako and Adam Durity performed this work
% while at Carnegie Mellon University.
% Copyright is held by the author/owner. Permission to make digital or hard
% copies of all or part of this work for personal or classroom use is granted
% without fee.
% Symposium on Usable Privacy and Security (SOUPS) 2015, July 22–24,
% 2015, Ottawa, Canada.
% website, or linked to from mobile app stores or mobile apps,
% to signs posted in public places to inform about CCTV cam-
% eras in operation. Even an LED indicating that a camera
% or microphone is active and recording constitutes a privacy
% notice, albeit one with limited information about the data
% practices associated with the recording. Providing notice
% about data practices is an essential aspect of data protec-
% tion frameworks and regulation around the world [57]. While
% transparency has been emphasized as an important practice
% for decades, existing privacy notices often fail to help users
% make informed choices. They can be lengthy or overly com-
% plex, discouraging users from reading them.
% Smartphones and mobile apps introduce additional pri-
% vacy issues as they support recording of sensor and behav-
% ioral information that enables inference of behavior patterns
% and profiling of users. Yet, comparatively smaller screens
% and other device restrictions constrain how users can be
% given notice about and control over data practices.
% The increasing adoption of wearable devices, such as smart
% watches or fitness trackers, as well as smart home devices,
% such as smart thermostats, connected light bulbs, or smart
% meters, represents a trend towards smaller devices that are
% even more constrained in terms of interaction capabilities,
% but are also highly connected with each other and the cloud.
% While providing notice and choice is still considered essential
% in the “Internet of Things” (IoT) [48, 74], finding appropriate
% and usable notice and choice mechanisms can be challenging.
% The challenges of providing usable privacy notice have
% been recognized by regulators and researchers. For instance,
% FTC chairwoman Edith Ramirez [107] stated in the IoT con-
% text: “In my mind, the question is not whether consumers
% should be given a say over unexpected uses of their data;
% rather, the question is how to provide simplified notice and
% choice.” An extensive body of research has studied usability
% issues of privacy notices (e.g., [14, 33, 64, 51]) and proposed
% improved notice interfaces (e.g., [34, 66, 67]), as well as tech-
% nical means to support them (e.g., [75, 127, 131]). Multi-
% stakeholder processes have been initiated in the wake of the
% White House’s proposed Consumer Bill of Rights [122] to
% tackle transparency and control issues of mobile privacy [92]
% and facial recognition [93]. While such efforts have resulted
% in guidance for notices in the context of particular systems,
% they have given little consideration to usability [14].
% Existing frameworks and processes for building privacy-
% friendly systems, such as Privacy by Design [36] or privacy
% impact assessments [136], focus on the analysis of a system’s
% data practices and less so on the design of notices. Even the
% OECD report on “making privacy notices simple” [94] basi-
% 1
% 2 2015 Symposium on Usable Privacy and Security USENIX Association
% cally states that one should design a simplified notice, con-
% duct usability tests, and deploy it – the crucial point of how
% to design a simplified notice is not addressed. Common pro-
% posals to improve the usability of privacy notices are the use
% of multi-layered notices [9, 26] or just-in-time notices [47].
% Despite the previous work on privacy notices, transparency
% tools, and privacy mechanisms, a system designer or devel-
% oper has very little guidance on how to arrive at a privacy
% notice design suitable and appropriate for their specific sys-
% tem and its respective characteristics. Existing best prac-
% tices are spread throughout the literature and have not pre-
% viously been organized into a comprehensive design frame-
% work. As a result, privacy notices are often hastily bolted
% on rather than well-integrated into a system’s interaction de-
% sign. Designers may not be aware of the many alternatives
% for designing usable privacy notices and therefore do not
% systematically consider the options. Furthermore, designers
% and researchers do not yet have a standard vocabulary for
% describing privacy notice options.
% In this paper, we make multiple contributions to ease the
% design of privacy notices and their integration into a system.
% The goal is to help developers embed privacy notices and
% choice options into their system design where relevant, with
% minimal disruption to the system’s interaction flow. First,
% we identify challenges, requirements, and best practices for
% the design of privacy notices. Based on a survey of existing
% literature and privacy notice examples, we develop a design
% space of privacy notices. This design space and its dimen-
% sions provide a systemization of knowledge and a taxonomy
% to foster understanding and reasoning about opportunities
% for privacy notices and controls. We demonstrate the util-
% ity of our design space by discussing existing privacy notice
% approaches in different domains.




\section{Privacy Challenges}

IoT is a composed of a complex web of architectures, applications and
technologies. In terms of architectures, it can be decomposed in three
layers: the perception layer, the network layer and the application or
business layer.

The perception layer, also known as the sensor layer, interacts with physical
objects and components via smart devices (RFID, sensors, actuators, and so on).
Its key objectives are to connect objects to the IoT network and to monitor,
collect, and analyze status information about these things using deployed
smart devices. This layer can often be unreliable, for instance with autonomous
vehicles where they find it hard to read road signs or to predict if certain
objects are inanimate or not, but this unreliability also brings privacy
even though the some of the data might be unusable. Noise can also be added in
this layer to provide extra privacy.

In the network layer there are many competing networks like ZigBee, Z-Wave,
Bluetooth Low Energy, LoRa, Wi-fi, etc., this layer is fragmented specially
in regards to wireless networks and that makes it very difficult to create
an IoT architecture that can use various networks and have the various
devices communicate with each other, even though interoperability is seen
as a very important factor in IoT. Some of these networks are open standard
protocols while others are proprietary and use different protocols of communication,
use different frequencies, different ranges and different data rates. When
creating an IoT architecture the designers often think of how to solve
specific problems and use what is best for the current needs, and the way
that IoT is fragmented doesn't help in providing progress.

The application layer receives data from the network layer and uses it to
execute essential services or operations. This layer, for example, can provide
the storage service to backup incoming data into a database or the analysis
service to analyze received data in order to predict the future state of
physical devices. This layer encompasses a wide range of applications,
each with its own set of requirements. A few examples are smart grids,
smart transportation, and smart cities.

% In the network layer there are many competing networks like ZigBee, Z-wave
% Bluetooth Low Energy, LoRa, Wi-fi, etc., this layer is fragmented specially
% in regards to wireless networks, and that makes it very difficult to create
% an IoT architecture that can use various networks and have the various
% devices communicate with each other, even though interoperability is seen
% as a very important factor in IoT. Some of these networks are free to use
% while others are proprietary and use different protocols of communication,
% use different frequencies, different ranges and different data rates. When
% creating an IoT architecture the designers often think of how to solve
% specific problems and use what is best for the current needs, and the way
% that IoT is fragmented doesn't help in providing progress.

% In the network layer there are many competing networks like ZigBee, Z-wave
% Bluetooth Low Energy, LoRa, Wi-fi, etc., this layer is fragmented specially
% in regards to wireless networks, and that makes it very difficult to create
% an IoT architecture that can use various networks and have the various
% devices communicate with each other, even though interoperability is seen
% as a very important factor in IoT. Some of these networks are free to use
% while others are proprietary and use different protocols of communication,
% use different frequencies, different ranges and different data rates. When
% creating an IoT architecture the designers often think of how to solve
% specific problems and use what is best for the current needs, and the way
% that IoT is fragmented doesn't help in providing progress.






According to Qu et al. \cite{Qu2018Privacy}, several significant barriers
remain, including the lack of a theoretical foundation, the trade-off
optimization between privacy and data value, and system isomerism
over-complexity. Because there are no mathematical foundations for IoT
structure design, IoT system designs are planned and executed using empirical
approaches, which have limitations in IoT development. Scientific theory
and quantitative analysis must enable trade-off optimization, yet, there
are multiple parties with diverse characteristics and requirements, making
this optimization highly challenging. A plethora of standards and protocols
add to the unneeded complexity of system isomerism. Ensuring effective
IoT applications while wasting as little resources as feasible implies
less resources available for privacy protection, however, lightweight
privacy protection cannot fulfill all of the criteria, and attackers can
exploit structural information to launch several concurrent attacks.
\\*[11pt]
\textbf{TODO: from here forward}

% According to Qu et al. \cite{Qu2018Privacy}, several substantial barriers
% remain, including the lack of a theoretical framework, the trade-off optimization
% between privacy and data utility, and system isomerism over-complexity.
% There are no mathematical foundations for IoT structure design, IoT system
% designs are planned and performed using empirical ways, the shortcomings of
% empirical methods limit IoT progress. To begin with, improving IoT performance
% entirely on human experience is tough. Second, developing privacy protection
% systems is difficult without theoretical guidance. Third, opponents may
% utilize this function to increase the success rate of their attacks. Trade-off
% optimization must be supported by scientific theory and quantitative analysis.

% Trade-off optimization has to be built on
% scientific theory and quantitative analysis. However, there are multiple
% parties with dynamic characteristics and diversified requirements, which
% greatly complicates this optimization. A massive number of standards
% and protocols facilitate the over-complication of the isomerism of systems.
% An over-complicated isomerism causes inconveniences to communication and
% system integration. Ensuring effective IoT applications without wasting
% too many resources leaves fewer resources for privacy protection. Nevertheless,
% lightweight privacy protection cannot satisfy all the requirements. In
% addition, adversaries can utilize the structural information to launch
% various and continuous attacks.

% Qu et al. highlight several key challenges that still need to be overcome namely the lack of a theoretical foundation, the trade-off optimization between privacy and data utility and system isomerism over-complexity. There are no mathematical foundations for IoT structure design, IoT system architectures are designed and implemented using empirical methods, the disadvantages of empirical methods limit the development of IoT. First, it is not easy to optimize IoT performance simply based on human experience. Second, it is difficult to implement privacy protection mechanisms without theoretical guidance. Third, adversaries can utilize this fact and increase the success rates of attacks. Trade-off optimization has to be built on scientific theory and quantitative analysis. However, there are multiple parties with dynamic characteristics and diversified requirements, which greatly complicates this optimization. Moreover, a lack of theoretical foundation causes non-uniform quantitative measurements and thereby introduces uncertainty into trade-off optimization. A massive number of standards and protocols facilitate the over-complication of the isomerism of systems. An over-complicated isomerism causes inconveniences to communication and system integration. Ensuring effective IoT applications without wasting too many resources leaves fewer resources for privacy protection. Nevertheless, lightweight privacy protection cannot satisfy all the requirements. In addition, adversaries can utilize the structural information to launch various and continuous attacks.

% The lack of theoretical foundation limits the development of IoT, and
% so it is necessary to bridge the gap from practice to theory with the
% utilization of multiple tools, like for instance, information theory, graph
% theory, machine learning, and convex optimization. When it comes to security,
% lightweight encryption can be used to ensure secure communication and data
% transmission among multiple and cross-layer networks, lightweight attribute-based
% encryption can be used to achieve proofs against attacks, especially collusion
% attacks, in addition to these, lightweight searchable encryption can be
% utilized to address query-based searchable content problems.

% Since the lack of theoretical foundation limits
% the development of IoT, it is necessary to bridge
% the gap from practice to theory. The theoretical
% methodology rather than empiricism facilitates the
% evolution and blossoming of IoT. We believe that
% there is no single area that can solve this problem,
% and cross-discipline methods are imperative. We
% need to utilize multiple tools, for instance, infor-
% mation theory, graph theory, machine learning,
% and convex optimization

% Leveraging lightweight encryption for ensur-
% ing secure communication and data transmission
% among multiple and cross-layer (cascade) networks
% is a promising methodology. Adversaries can launch
% eavesdropping-based attacks during the data trans-
% mission process. Due to the extensive scale of IoT,
% traditional cryptography methods cannot solve this
% problem efficiently. Therefore, lightweight encryp-
% tion can be applied to ensure efficiency. We can
% also use lightweight attribute-based encryption to
% achieve proofs against attacks, especially collusion
% attacks. In addition, lightweight searchable encryp-
% tion can be utilized to address query-based search-
% able content problems.

% Personalized privacy can provide satisfactory pri-
% vacy protection while reducing the overall privacy
% resource budget. Existing models mostly focus on
% uniform privacy, which applies a constant privacy
% level to all entities. This is neither practical nor
% resource-efficient. However, personalized privacy
% can address this problem by assigning different
% privacy levels based on various requirements. For
% example, in CPSSs, we can first model a system as
% a graph and then assign the personalized privacy
% level according to the social distances between
% two users. We can use a sigmoid function to map
% the social distance to the privacy level. The ratio-
% nale behind this is that users tend to release more
% accurate data to friends but avoid data disclosure
% to unfamiliar users. The privacy level increases
% slowly at first but is followed by a sudden increase
% for users at relatively long distances. Then the pri-
% vacy level converges to a constant after the social
% distance exceeds a threshold.

% Trade-off denotes the balance between privacy
% and data/service utility since both of these indi-
% ces affect a system's quality of service. Howev-
% er, this trade-off is not linear or even polynomial
% sometimes, which makes it difficult to measure or
% analyze quantitatively. Normally, we use entropy
% from information theory to measure privacy loss.
% Analogously, we leverage the probability of being
% re-identified or the mean square error to judge
% the data utility. Nevertheless, the measurement is
% the first phase, while optimization is the final step.
% To obtain the optimized trade-off, game theory
% is a quite useful tool. Game theory leverages the
% characteristics of the confrontation between the
% data curator and the adversary. If one can accu-
% rately define the action and payoff mathematical-
% ly, we can derive the Nash equilibrium based on
% game theory, which denotes the optimized trade-
% off. Moreover, IoT services are long-term and sta-
% ble, meaning that they face continuous attacks
% from adversaries. Therefore, we can extend game
% theory into Markov decision processes and use
% machine learning methods to derive the overall
% Nash equilibrium with long-term protection

% New technologies, such as artificial intelligence,
% quantum computing, autonomous vehicles, and
% biocomputing, are emerging. These technologies
% deeply integrate with IoT, especially as they rely on
% massive volumes of data. However, new technolo-
% gies can be a double-edged sword. On one hand,
% they have positive mutual impacts on each other.
% On the other hand, privacy and security issues
% are more complicated, and difficult to model and
% thereby control. Therefore, we need to devise a
% uniform privacy protection framework that can be
% implemented by all such emerging technologie

\section{Methodology}

This paper conducted a systematic literature review of the most relevant papers
discussing methodologies and techniques for the protection of users' privacy
data with special focus on IoT systems. For this SLR, this paper considered focusing
only on papers from the last 12 years, from 2010 until 2022, since papers before
then become out of date with the evolution of technology. In this SLR, it was reviewed 100
(*\textbf{\color{red}number to be adjusted}) papers published in top computer
science, security, privacy and software engineering outlets.

This paper followed Keshav's three-pass approach \cite{KeshavHow} when choosing which
papers to read fully and which ones to ignore, first the title would be read,
then the abstract, the introduction and conclusion and briefly skim the rest of the paper
and then decide if it was worth reading any further, the focal point in this
phase was ans wering the following question: does the paper present a new
methodology or interesting angle to tackle users' privacy concerns? Only then
the document would be read in it's entirety while ignoring any tables, figures,
images or graphs. If the paper failed to present any interesting idea, approach,
or technique it would be discarded, but if not, it would be read carefully
from the beginning again in order to fully understand what it presents.

The proposed methodology is composed of two phases, the first phase consists
of making a study on the region's general concern with their privacy when using
and interacting with IoT devices, their knowledge of privacy rights, what they
do to protect their privacy rights. One one side the objective of this study
consist in demystifying the privacy paradox in the region and gather information
about their idea to solve this problem with respect to IoT devices. The second
phase consists in doing an application that can detect IoT devices nearby the
user with at least a 10 meters radius. The application should do the following
when detecting a device:
1. it should show some information about the device;
2. it should categorize the device;
3. it should provide the user with privacy options, if the device allows the
user to decline data harvesting.
This application at first sight might appear to be a mere privacy assistant but
it's not, because IoT assistants merely choose what privacy options the user
first sets and maintains it for every other application that the user might use.
The proposed app doesn't have the objective to conform to the user's preferred
privacy choices, it merely informs the user about nearby IoT devices and can
provide the user with privacy options. But the main objective is creating
awareness in individuals about the various devices that are around and make the
user questions their choices.

\section{Study}

This study was done to try to understand people's perception of IoT and their
privacy practices online. It was partially based in a study done in the Philippines
by the government in the context of their privacy act of 2012, this was the second
survey done on the population. This survey was done through the internet, it
was created in google forms, this way it is guaranteed to reach most people in
the country.

\section{Application}

I created something and...

\section{Discussion}

The study found the following... The app...

\section{Future work}

Although there exist some hardware devices that can detect some devices
on some networks, like ZigBee or Bluetooth LE, namely IoT sniffers and
there exist some georeferencing applications that try to pinpoint certain
IoT devices, there is still a need for some kind of device or framework
that is network agnostic and can detect where the devices are located and
what kind of data the IoT devices that are around it are collecting.
This gadget should also be capable of informing the users about the privacy
notices of the devices and what can the users do to safeguard their data.
The IoT sniffers that are available are primarily used in the detection of
problems in the communication of devices in the network or to solve problems
of interoperability between different IoT networks. There are many obstacles
that impede the creation of such a gadget and the fact that it still doesn't
exist anything like it shows that maybe there is not enough interest from
users or researchers to focus on such an endeavour or the complexity
of such a task is greater than the rewards.

\section{Conclusion}

In this thesis I explored people's perception of privacy of IoT systems and
made an application that aims to create more awareness in users about their
environment and the IoT devices that inhabit it.

\section*{Acknowledgment}

% I would like to acknowledge the following people: Person 1, Person 2, Person 3.
The preferred spelling of the word ``acknowledgment'' in America is without
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B.
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor
acknowledgments in the unnumbered footnote on the first page.

\bibliographystyle{IEEEtran}
\bibliography{assets/references}

\end{document}
